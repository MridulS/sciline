{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Data analysis workflows are often complex and involve many steps.\n",
    "For example, we may want to:\n",
    "\n",
    "1. Define or import functions to use for processing.\n",
    "2. Define parameters for processing.\n",
    "3. Load the data and apply functions to it.\n",
    "\n",
    "There are a couple of problems with this:\n",
    "\n",
    "- In complex workflows, we are forced to write a lot of boilerplate code to load data, apply functions, and save results.\n",
    "  This is tedious and error-prone, e.g., since the order of function calls may be wrong or the wrong data and parameters may be passed to a function.\n",
    "  This makes it hard to focus on the actual analysis.\n",
    "- In Jupyter notebooks, the order of cell execution is not clear.\n",
    "  This frequently leads to errors that are hard to track down in retrospect and analysis results that are hard to reproduce.\n",
    "\n",
    "In traditional software development some of these problems are addressed by writing unit tests or integration tests.\n",
    "However, in our experience this is challenging to do properly for data analysis workflows:\n",
    "\n",
    "- Workflows are often interactive and under active development.\n",
    "  Very frequently part of or all of the workflow is written in a Jupyter notebook.\n",
    "- It is very time consuming to setup good test data for a workflow with good *fidelity*, i.e., test data that will actually allow you to catch errors in your workflow.\n",
    "\n",
    "A very simplified model workflow of such as traditional is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename: str) -> dict:\n",
    "    \"\"\"Load the data from the filename.\"\"\"\n",
    "    return {'data': [1, 2, float('nan'), 3], 'meta': {'filename': filename}}\n",
    "\n",
    "\n",
    "def clean(raw_data: dict) -> list:\n",
    "    \"\"\"Clean the data, removing NaNs.\"\"\"\n",
    "    import math\n",
    "\n",
    "    return [x for x in raw_data['data'] if not math.isnan(x)]\n",
    "\n",
    "\n",
    "def process(data: list, param: float) -> float:\n",
    "    \"\"\"Process the data, multiplying the sum by the scale factor.\"\"\"\n",
    "    return sum(data) * param\n",
    "\n",
    "\n",
    "filename = 'data.txt'\n",
    "scale_factor = 2.0\n",
    "\n",
    "raw_data = load(filename)\n",
    "cleaned_data = clean(raw_data)\n",
    "result = process(cleaned_data, scale_factor)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain types, providers, and pipelines\n",
    "\n",
    "Sciline uses a different approach.\n",
    "We can rewrite the model workflow from above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "import sciline\n",
    "\n",
    "# 1. Define domain types\n",
    "\n",
    "Filename = NewType('Filename', str)\n",
    "RawData = NewType('RawData', dict)\n",
    "CleanedData = NewType('CleanedData', list)\n",
    "ScaleFactor = NewType('ScaleFactor', float)\n",
    "Result = NewType('Result', float)\n",
    "\n",
    "\n",
    "# 2. Define providers\n",
    "\n",
    "\n",
    "def load(filename: Filename) -> RawData:\n",
    "    \"\"\"Load the data from the filename.\"\"\"\n",
    "    return {'data': [1, 2, float('nan'), 3], 'meta': {'filename': filename}}\n",
    "\n",
    "\n",
    "def clean(raw_data: RawData) -> CleanedData:\n",
    "    \"\"\"Clean the data, removing NaNs.\"\"\"\n",
    "    import math\n",
    "\n",
    "    return [x for x in raw_data['data'] if not math.isnan(x)]\n",
    "\n",
    "\n",
    "def process(data: CleanedData, param: ScaleFactor) -> Result:\n",
    "    \"\"\"Process the data, multiplying the sum by the scale factor.\"\"\"\n",
    "    return sum(data) * param\n",
    "\n",
    "\n",
    "# 3. Create pipeline\n",
    "\n",
    "providers = [load, clean, process]\n",
    "params = {Filename: 'data.txt', ScaleFactor: 2.0}\n",
    "pipeline = sciline.Pipeline(providers, params=params)\n",
    "\n",
    "pipeline.compute(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the task graph for computing `Result`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.visualize(Result, graph_attr={'rankdir': 'LR'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have set up a data pipeline in three steps:\n",
    "\n",
    "1. Define the domain types, i.e., unambiguous types specific to the workflow or problem.\n",
    "   Sciline uses \"domain types\" as a way to identify inputs and outputs of processing steps.\n",
    "2. Define the providers.\n",
    "   Sciline uses \"providers\" to obtain domain objects, i.e., instances of required domain types.\n",
    "   Each provider is either a callable (such as a function) that can compute the required domain object (from other domain objects) or simply a domain object.\n",
    "3. Create the pipeline from a list of callables (that require domain objects and compute derived domain objects) and parameters (existing domain objects).\n",
    "\n",
    "The pipeline sets up a directed acyclic graph (DAG) of processing steps.\n",
    "This is done by inspecting the type-hints of the callables, i.e., the required domain objects.\n",
    "\n",
    "If you are familiar with the concept of *dependency injection* you can think of the pipeline as a dependency injection container.\n",
    "Each provider declares its dependencies (required domain objects) via *type hints* and the pipeline resolves them.\n",
    "   \n",
    "The pipeline can then be used to compute results or to visualize the structure of the workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
